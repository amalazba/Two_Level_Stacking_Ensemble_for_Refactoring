{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxaBPseTtWPD"
   },
   "source": [
    "# **Experiment Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.environ['PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKRhL2uotWPO"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------  Datasets\n",
    "\n",
    "CLASS_LEVEL_REFACTORINGS = [ \"Extract Class.csv\", \"Extract Interface.csv\", \"Extract Subclass.csv\", \"Extract Superclass.csv\", \n",
    "                               \"Move And Rename Class.csv\", \"Move Class.csv\",\"Rename Class.csv\" ]\n",
    "\n",
    "METHOD_LEVEL_REFACTORINGS = [\"Extract And Move Method.csv\", \"Extract Method.csv\", \"Inline Method.csv\",  \"Move Method.csv\",\n",
    "                                \"Pull Up Method.csv\",  \"Push Down Method.csv\", \n",
    "                                \"Rename Method.csv\"]\n",
    "\n",
    "VARIABLE_LEVEL_REFACTORINGS = [ \"Extract Variable.csv\", \"Inline Variable.csv\", \"Parameterize Variable.csv\", \"Rename Parameter.csv\",\n",
    "                                 \"Rename Variable.csv\", \"Replace Variable With Attribute.csv\"]\n",
    "\n",
    "\n",
    "#\"Extract Method.csv\", 'Rename Parameter.csv', \n",
    "short = [ 'Move And Rename Class.csv', 'Extract And Move Method.csv']\n",
    "\n",
    "fnames = METHOD_LEVEL_REFACTORINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4H1TQDbtWPP"
   },
   "outputs": [],
   "source": [
    "# ----------------  Experiment Parameters\n",
    "CV  = '10SCV'    # cross validation (CS) values: 10, 10SCV, 10x10, 5x2, LOO\n",
    "dpi = 600\n",
    "\n",
    "F = False\n",
    "T = True\n",
    "\n",
    "#-----------------  Feature Selection\n",
    "_GainRatio  = T\n",
    "_PCA        = F\n",
    "num_feature = 0.95\n",
    "\n",
    "#-----------------  File Output\n",
    "_WriteFlag = T\n",
    "\n",
    "#-----------------  Visualization\n",
    "_Bar = F # Bar charts\n",
    "_ROC = F # Roc curve\n",
    "_Box = T # Boxplots\n",
    "_ESD = T # ScottKnottESD\n",
    "\n",
    "#-----------------  Hyperparameters\n",
    "_GridSearch  = F\n",
    "_CompIndTune = F # Compare_Tune_Vs_Default for indivsual model\n",
    "_CompTenTune = F # Compare_Tune_Vs_Default for Tree-based ensemble models\n",
    "\n",
    "#----------------- Feature transformation \n",
    "_MinMax = T  # Enable Min-Max Normalization: Scales and translates each feature individually such that it is in the given range on the training set between zero and one.\n",
    "_Scaler = F  # Enable Standard Scaling: Standardize features by removing the mean and scaling to unit variance.\n",
    "_MaxAbs = F  # Enable MaxAbs Scaling: Scale each feature by its maximum absolute value to ensure the data range is [-1, 1].\n",
    "_Robust = F  # Enable Robust Scaling: Scale features using statistics that are robust to outliers.\n",
    "_Quanti = F  # Possibly indicates Quantile Transformation: Transform features to follow a uniform or a normal distribution.\n",
    "_BoxCox = F  # Enable Box-Cox Transformation: Transform features to be more Gaussian-like.\n",
    "_LogTra = F  # Enable Log Transformation: Apply natural logarithm transformation to the features.\n",
    "\n",
    "\n",
    "#----------------- Sampling \n",
    "_Sampling       = T      # Apply sampling techniques\n",
    "_SMOTE          = F\n",
    "_UnderSampling  = T\n",
    "_max_rows      = None\n",
    "\n",
    "#-----------------  Model Selection\n",
    "incInd    = T      # Indivisual models\n",
    "incBag    = F      # Bagging ensemble\n",
    "incBos    = F      # Boosting ensemble\n",
    "incTen    = F      # Tree-based ensembles\n",
    "incSta    = T      # Stacking ensemble\n",
    "incTenSta = T      # Tree-based Stacking ensemble\n",
    "incVot    = T      # Voting ensemble\n",
    "\n",
    "#-----------------  Comparison Flags\n",
    "_CompAll  = T   # Compare all models\n",
    "_CompInd  = F   # Comapre indivsual models\n",
    "_StatFlag = T   # Statistical analysis\n",
    "\n",
    "\n",
    "#----------------- Select metrics\n",
    "statMetric = 'Accuracy'\n",
    "boxMetrics = ['Accuracy', 'F1-score'  , 'F1-micro'  , 'F1-weight' , 'Brier' , 'AUC' ]\n",
    "barMetric = ['Accuracy']\n",
    "ESDmetric = 'AUC'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUtjI-gMxgIu"
   },
   "source": [
    "# **G-Drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6iSLpaERPPt4",
    "outputId": "e9936c08-8cdc-40f1-b3fe-590c9ad9fcf3"
   },
   "outputs": [],
   "source": [
    "#%%capture\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "km2boD6zspff",
    "outputId": "5d60cd94-d6aa-4237-c7ac-4fdab09295ae"
   },
   "outputs": [],
   "source": [
    "#cd drive/My Drive/MachineLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def namestr(obj, namespace):\n",
    "    name = [name for name in namespace if namespace[name] is obj]\n",
    "    name = name[0]\n",
    "    \n",
    "    return name\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def get_output_dir():\n",
    "    \n",
    "    ref_type = namestr(fnames, globals())\n",
    "    version = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") \n",
    "\n",
    "    if _GainRatio:\n",
    "        return ref_type + '/GainRatio '+version + '/'\n",
    "    elif _PCA:\n",
    "        return ref_type + '/PCA '+version + '/'\n",
    "    else:\n",
    "        return ref_type + '/NoFS '+version + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YIn2hz5Eb9xO",
    "outputId": "3a26b7ed-d4a6-4a4f-e063-8de2756ab813"
   },
   "outputs": [],
   "source": [
    "# ----------------  Output directory name\n",
    "output_dir = get_output_dir() #'Run1/'+ datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "dataPath = 'Datasets/'\n",
    "prePath = 'Datasets/Preprocess/'\n",
    "indPath = output_dir + 'IndivResults/'\n",
    "bagPath = output_dir +'BagResults/'\n",
    "bosPath = output_dir +'BosResults/'\n",
    "tenPath = output_dir +'TenResults/'\n",
    "staPath = output_dir +'StackResults/'\n",
    "tenstaPath = output_dir +'TenStackResults/'\n",
    "votPath = output_dir +'VoteResults/'\n",
    "comPath = output_dir +'CompareResults/'\n",
    "\n",
    "import os\n",
    "\n",
    "directories = [\n",
    "    'Datasets/Preprocess',\n",
    "    output_dir +'IndivResults',\n",
    "    #output_dir +'BagResults',\n",
    "    #output_dir +'BosResults',\n",
    "    output_dir +'TenResults',\n",
    "    output_dir +'StackResults',\n",
    "    output_dir +'TenStackResults',\n",
    "    output_dir +'VoteResults',\n",
    "    output_dir +'CompareResults'\n",
    "]\n",
    "\n",
    "for dir_name in directories:\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    print(f\"Directory '{dir_name}' created\" if not os.path.exists(dir_name) else f\"Directory '{dir_name}' already exist\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "po7IpsojXDz_"
   },
   "source": [
    "# **Installs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGYiOFTTvWgo"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#pip install info_gain\n",
    "#!pip install xgboost\n",
    "#!pip install catboost\n",
    "#!pip install seaborn\n",
    "#!pip install imblearn\n",
    "#!pip install scipy==1.11.4\n",
    "#!pip install openpyxl\n",
    "#!pip install scikit-learn-intelex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zhhQzssP8P6"
   },
   "source": [
    "# **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OsT4H-FhP0MF"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "from sklearnex import patch_sklearn, config_context\n",
    "\n",
    "patch_sklearn()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "import time\n",
    "from   datetime import date\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sys.setrecursionlimit(1000)\n",
    "sns.set(style='ticks', palette='muted')\n",
    "\n",
    "from sklearn.preprocessing       import LabelEncoder, MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler, QuantileTransformer, PowerTransformer, FunctionTransformer\n",
    "from sklearn.impute              import SimpleImputer\n",
    "from sklearn.decomposition       import PCA\n",
    "from info_gain                   import info_gain\n",
    "from imblearn.over_sampling      import SMOTE\n",
    "from imblearn.under_sampling     import RandomUnderSampler, EditedNearestNeighbours\n",
    "from scipy                       import interp\n",
    "from scipy.stats                 import shapiro, mannwhitneyu, ttest_ind, wilcoxon, ttest_rel\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from statistics                  import mean\n",
    "from random                      import randint\n",
    "from enum                        import Enum\n",
    "from collections                 import Counter\n",
    "\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble     import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, StackingClassifier, ExtraTreesClassifier, HistGradientBoostingClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, Perceptron\n",
    "from sklearn.calibration  import CalibratedClassifierCV\n",
    "from sklearn.svm          import SVC, LinearSVC, OneClassSVM\n",
    "from sklearn.naive_bayes  import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.tree         import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.neighbors    import KNeighborsClassifier, RadiusNeighborsClassifier, NearestCentroid\n",
    "from sklearn.mixture      import GaussianMixture\n",
    "from xgboost              import XGBClassifier\n",
    "from catboost             import CatBoostClassifier\n",
    "from sklearn.neural_network           import MLPClassifier\n",
    "from sklearn.gaussian_process         import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern\n",
    "from sklearn.discriminant_analysis    import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics.pairwise         import rbf_kernel, linear_kernel\n",
    "\n",
    "from sklearn.metrics         import accuracy_score, f1_score, roc_curve, roc_auc_score, auc, brier_score_loss, confusion_matrix #plot_roc_curve \n",
    "from sklearn.model_selection import cross_validate, cross_val_score, StratifiedKFold, KFold, LeaveOneOut, StratifiedShuffleSplit, RepeatedStratifiedKFold, GridSearchCV, train_test_split\n",
    "from sklearn.metrics         import make_scorer, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cn0s02tOrHdt"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------  ScottKnottESD installation\n",
    "if _ESD :\n",
    "\n",
    "  #!pip install rpy2\n",
    "  #!pip install pyreadr\n",
    "  #!pip install pandas plotnine\n",
    "\n",
    "  # import rpy2.robjects as ro\n",
    "  # package_name = \"ScottKnottESD\"\n",
    "  # try:\n",
    "  #     sk = importr(package_name)\n",
    "  # except:\n",
    "  #     ro.r(f'install.packages(\"{package_name}\")')\n",
    "  #     sk = importr(package_name)\n",
    "\n",
    "  import rpy2.robjects.packages as rpackages\n",
    "  from rpy2.robjects.vectors  import StrVector\n",
    "\n",
    "  from rpy2.robjects.packages import importr\n",
    "  from rpy2.robjects import r, pandas2ri\n",
    "  from plotnine import *\n",
    "  pandas2ri.activate()\n",
    "    \n",
    "  pandas2ri.activate()\n",
    "\n",
    "  utils = rpackages.importr('utils')  # import R's utility package\n",
    "  utils.chooseCRANmirror(ind=1)       # select the first mirror in the list\n",
    "  utils.install_packages(StrVector('ScottKnottESD'))\n",
    "  \n",
    "  packnames = [\"ScottKnottESD\"]\n",
    "\n",
    "  # Selectively install what needs to be installed.\n",
    "  names_to_install = [x for x in packnames if not rpackages.isinstalled(x)]\n",
    "  if len(names_to_install) > 0:\n",
    "    utils.install_packages(StrVector(names_to_install))\n",
    "\n",
    "  try:\n",
    "      sk = importr('ScottKnottESD')\n",
    "  except:\n",
    "      #ro.r(f'install.packages(\"{package_name}\")')\n",
    "      #sk = importr(package_name)\n",
    "      print('Cant download ScottKnottESD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArKzUY7XBgbi"
   },
   "source": [
    "# **Utilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhYkUggCtFBf"
   },
   "outputs": [],
   "source": [
    "class colors:\n",
    "    HEADER = '\\033[95m'\n",
    "    BLUE   = '\\033[94m'\n",
    "    CYAN   = '\\033[96m'\n",
    "    GREEN  = '\\033[92m'\n",
    "    WARN   = '\\033[93m'\n",
    "    RED    = '\\033[91m'\n",
    "    ENDC   = '\\033[0m'\n",
    "    BOLD   = '\\033[1m'\n",
    "    UNDER  = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T_gkOcnhlzkF"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "def printStart():\n",
    "\n",
    "  print( colors.BOLD + colors.BLUE + '========================================================================')\n",
    "  print( colors.BOLD + colors.BLUE + '\\t\\t\\t\\tStart ')\n",
    "  print( colors.BOLD + colors.BLUE + '========================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uy5eYysD_HY6"
   },
   "outputs": [],
   "source": [
    "def printString(str):\n",
    "\n",
    "  print( colors.BOLD + colors.BLUE + '========================================================================')\n",
    "  print( colors.BOLD + colors.BLUE + '\\t\\t\\t' + str)\n",
    "  print( colors.BOLD + colors.BLUE + '========================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWU8KdUlBhId"
   },
   "outputs": [],
   "source": [
    "def printDone():\n",
    "\n",
    "  print( colors.BOLD + colors.BLUE + '========================================================================')\n",
    "  print( colors.BOLD + colors.BLUE + '\\t\\t\\t\\tDONE')\n",
    "  print( colors.BOLD + colors.BLUE + '\\t\\t\\t\\tTime (minutes): ', round((time.time() - start_time)/60))\n",
    "  print( colors.BOLD + colors.BLUE + '========================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c83UqM9Qozs6"
   },
   "outputs": [],
   "source": [
    "def Save_CSV (result, fname, mode='w'):\n",
    "  if not _WriteFlag: return\n",
    "\n",
    "  if not os.path.isfile(fname) : result.to_csv (fname, header='column_names')\n",
    "  else :\n",
    "    if mode == 'w' : result.to_csv (fname, mode=mode, header='column_names')\n",
    "    if mode == 'a' : result.to_csv (fname, mode=mode, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jd9L-hlXkSyr"
   },
   "outputs": [],
   "source": [
    "def Save_Excel (result, fname, mode='w'):\n",
    "  if not _WriteFlag: return\n",
    "\n",
    "  if not os.path.isfile(fname) : result.to_excel (fname, header='column_names')\n",
    "  else :\n",
    "    if mode == 'w' : result.to_excel (fname, header='column_names')\n",
    "    if mode == 'a' : result.to_excel (fname, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10PYKvfDFOet"
   },
   "outputs": [],
   "source": [
    "def Save_File (result, fname, mode='w'):\n",
    "  if not _WriteFlag: return\n",
    "\n",
    "  f = open (fname, mode)\n",
    "  f.write (result)\n",
    "  f.close ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RzGLxylfk0G2"
   },
   "outputs": [],
   "source": [
    "def getToday ():\n",
    "  return str(date.today())\n",
    "\n",
    "def getTime ():\n",
    "  t = time.localtime()\n",
    "  current_time = time.strftime('%H-%M-%S', t)\n",
    "  return current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HlxNJjY6wtab"
   },
   "outputs": [],
   "source": [
    "def getPath (type):\n",
    "\n",
    "  path = ''\n",
    "\n",
    "  if   type == 'Preprocess'    : path = prePath\n",
    "  elif type == 'Individual'    : path = indPath\n",
    "  elif type == 'Bagging'       : path = bagPath\n",
    "  elif type == 'Boosting'      : path = bosPath\n",
    "  elif type == 'TreeBased'     : path = tenPath\n",
    "  elif type == 'Stacking'      : path = staPath\n",
    "  elif type == 'TreeStacking'  : path = tenstaPath\n",
    "  elif type == 'Voting'        : path = votPath\n",
    "  elif type == 'Compare'       : path = comPath\n",
    "\n",
    "  return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2w5ywIN9qmTN"
   },
   "outputs": [],
   "source": [
    "def getDatasetName (fname):\n",
    "\n",
    "  fname = fname.replace ('Datasets/','')\n",
    "  fname = fname.replace ('Preprocess/','')\n",
    "  fname = fname.replace ('IndivResults/','')\n",
    "  fname = fname.replace ('VoteResults/','')\n",
    "  fname = fname.replace ('BagResults/','')\n",
    "  fname = fname.replace ('BosResults/','')\n",
    "  fname = fname.replace ('TenResults/','')\n",
    "  fname = fname.replace ('StackResults/','')\n",
    "  fname = fname.replace ('TenStackResults/','')\n",
    "  fname = fname.replace ('CompareResults/','')\n",
    "  fname = fname.replace (output_dir,'')\n",
    "  fname = fname.replace ('_IndxBag','')\n",
    "  fname = fname.replace ('_IndxBos','')\n",
    "  fname = fname.replace ('_IndxTen','')\n",
    "  fname = fname.replace ('_IndxStack','')\n",
    "  fname = fname.replace ('_IndxVote','')\n",
    "  fname = fname.replace ('_IndxTune','')\n",
    "  fname = fname.replace ('_TenxTune','')\n",
    "  fname = fname.replace ('_Clean','')\n",
    "  fname = fname.replace ('_results','')\n",
    "  fname = fname.replace ('PCA','')\n",
    "  fname = fname.replace ('_','')\n",
    "  fname = fname.replace ('Ten','')\n",
    "  fname = fname.replace ('.csv','')\n",
    "\n",
    "  return fname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGh01lXVhBnK"
   },
   "source": [
    "# **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ON4Y-77JBlHP"
   },
   "outputs": [],
   "source": [
    "def Dataset_Preprocess (fname):\n",
    "    \n",
    "  start_time = time.time()\n",
    "    \n",
    "  fname = dataPath + fname\n",
    "\n",
    "  fname = Clean_Dataset (fname)\n",
    "\n",
    "  if   _Sampling    : fname = Balance_Data(fname)\n",
    "      \n",
    "  if   _GainRatio : fname = FS_GainRatio (fname)\n",
    "  elif _PCA       : fname = FS_PCA (fname, num_feature)\n",
    "\n",
    "\n",
    "      \n",
    "  logger.info(f\"Preprocessing time: {time.time() - start_time:.3f} seconds\")\n",
    "\n",
    "  return fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3vOMwBhhWp6t"
   },
   "outputs": [],
   "source": [
    "def prepareVars(fname):\n",
    "\n",
    "  dataset = pd.read_csv (fname)\n",
    "  ind = dataset.iloc[:, 0:-1]\n",
    "  dep = dataset.iloc[:,-1]\n",
    "  indTun, depTun = None, None\n",
    "\n",
    "  if _GridSearch:\n",
    "    ind, indTun, dep, depTun = train_test_split(ind, dep, test_size=0.1,  random_state=1, stratify=dep)\n",
    "    ind = ind.reset_index(drop = True)\n",
    "    dep = dep.reset_index(drop = True)\n",
    "\n",
    "  return ind, dep, indTun, depTun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c726OSqMJF0S"
   },
   "source": [
    "## **Clean Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JN_N7ugZapOO"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------  Fun: Handle Missing Values\n",
    "\n",
    "def Clean_Dataset (fname):\n",
    "\n",
    "  dataset = Read_DataSet (fname)\n",
    "  fname   = fname.replace(dataPath, prePath)\n",
    "\n",
    "  dataset = Handle_Missing (dataset)\n",
    "\n",
    "  dataset = Feature_Transformation (dataset)\n",
    "\n",
    "  fname = fname.replace('.csv','') + '_Clean.csv'\n",
    "  dataset.to_csv (fname, ',', index = False)\n",
    "\n",
    "  return fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PlwoeSw9dGPe"
   },
   "outputs": [],
   "source": [
    "def Read_DataSet (fname) :\n",
    "\n",
    "    missing = ['NA', 'n/a', 'na', '--']\n",
    "    dataset = pd.read_csv (fname, na_values = missing)\n",
    "\n",
    "    # Log the initial number of rows and columns in the dataset\n",
    "    initial_rows, initial_cols = dataset.shape\n",
    "    logger.info(f\"Initial dataset shape: {initial_rows} rows, {initial_cols} columns\")\n",
    "    \n",
    "    # Remove duplicate rows from the dataset\n",
    "    dataset.drop_duplicates(inplace=True)  # 'inplace=True' modifies the dataset directly\n",
    "    \n",
    "    # Apply 'nunique' across columns to find the number of unique values in each column\n",
    "    nunique = dataset.apply(pd.Series.nunique)\n",
    "    \n",
    "    # Identify columns where the number of unique values is exactly 1\n",
    "    colsDrop = nunique[nunique == 1].index\n",
    "    dataset.drop(colsDrop, axis=1, inplace=True)\n",
    "    \n",
    "    if len(colsDrop) > 0:\n",
    "        logger.info(f\"Columns to drop: {len(colsDrop)} {list(colsDrop)}\")\n",
    "    else:\n",
    "         logger.info(\"No columns to drop.\")\n",
    "    \n",
    "    final_rows, final_cols = dataset.shape\n",
    "    logger.info(f\"Final dataset shape after dropping columns and remove duplicates: {final_rows} rows, {final_cols} columns\")\n",
    "\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cahaKyumaijG"
   },
   "outputs": [],
   "source": [
    "def Handle_Missing (dataset) :\n",
    "\n",
    "  ind = dataset.iloc[:, 0:-1].values\n",
    "\n",
    "  imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "  imputer = imputer.fit(ind[:, 1:])\n",
    "  ind[:, 1:] = imputer.transform(ind[:, 1:])\n",
    "\n",
    "  dataset.iloc[:, 0:-1] = ind\n",
    "\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "im_0Ls7JZg6S"
   },
   "outputs": [],
   "source": [
    "def Feature_Transformation (dataset) :\n",
    "\n",
    "  ind = dataset.iloc[:, 0:-1]\n",
    "  dep = dataset.iloc[:,-1]\n",
    "\n",
    "  if   _MinMax : newInd = MinMaxScaler().fit_transform(ind.values)\n",
    "  elif _Scaler : newInd = StandardScaler().fit_transform(ind.values)\n",
    "  elif _MaxAbs : newInd = MaxAbsScaler().fit_transform(ind.values)\n",
    "  elif _Robust : newInd = RobustScaler().fit_transform(ind.values)\n",
    "  elif _Quanti : newInd = QuantileTransformer().fit_transform(ind.values)\n",
    "  elif _BoxCox : newInd = PowerTransformer(method='box-cox').fit_transform(ind.values)\n",
    "  elif _LogTra : newInd = FunctionTransformer(np.log1p, validate=True).fit_transform(ind.values)\n",
    "\n",
    "  encodeDep = LabelEncoder()\n",
    "  newDep = encodeDep.fit_transform(dep.values)\n",
    "\n",
    "  newDS = pd.concat([pd.DataFrame(newInd), pd.DataFrame(newDep)], axis=1)\n",
    "  newDS.columns = dataset.columns\n",
    "\n",
    "  return newDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TklRPkN7B-d"
   },
   "source": [
    "## **Imbalance Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "an3btx0R7XCd"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------  Check if dataset is imbalanced\n",
    "\n",
    "def Check_Imbalance(fname):\n",
    "\n",
    "  imbFlag = False\n",
    "  threshold = 10\n",
    "  dataset = pd.read_csv (fname)\n",
    "  dep = dataset.iloc[:,-1]\n",
    "\n",
    "  count = dep.value_counts()\n",
    "  falseRatio = (count[0] / count.sum()) * 100\n",
    "  trueRatio = (count[1] / count.sum()) * 100\n",
    "\n",
    "  if falseRatio < threshold or trueRatio < threshold:\n",
    "    imbFlag = True\n",
    "\n",
    "  return imbFlag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pwo3pKyS7Ieq"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------  Balance Imbalanced Data\n",
    "\n",
    "def Balance_Data (fname):\n",
    "\n",
    "  dataset = pd.read_csv (fname)\n",
    "\n",
    "  ind = dataset.iloc[:, 0:-1]\n",
    "  dep = dataset.iloc[:,-1]\n",
    "  logger.info(f\"Instances before balancing: {Counter(dep)}\")\n",
    "\n",
    "  # Count the instances of each class after NCR\n",
    "  unique, counts = np.unique(dep, return_counts=True)\n",
    "  class_counts = dict(zip(unique, counts))\n",
    "\n",
    "\n",
    "  if _SMOTE:\n",
    "      sm = SMOTE(random_state=42)\n",
    "  elif _UnderSampling:\n",
    "      #enn = EditedNearestNeighbours(sampling_strategy='all', n_neighbors=3)\n",
    "      #ind, dep = enn.fit_resample(ind, dep)\n",
    "      if _max_rows == None: \n",
    "          sm = RandomUnderSampler()\n",
    "      elif  all(count > _max_rows for count in class_counts.values()):\n",
    "          sm = RandomUnderSampler(sampling_strategy={key: _max_rows for key in class_counts.keys()})\n",
    "      else: \n",
    "          sm = RandomUnderSampler()\n",
    "\n",
    "  newInd, newDep = sm.fit_resample(ind, dep)\n",
    "  logger.info(f\"Instances after balancing: {Counter(newDep)}\")\n",
    "\n",
    "  balanced = pd.concat([pd.DataFrame(newInd), pd.DataFrame(newDep)], axis=1)\n",
    "  balanced.columns = dataset.columns\n",
    "\n",
    "  fname = fname.replace('.csv','') + \"_Balanced.csv\"\n",
    "  balanced.to_csv (fname, \",\", index = False)\n",
    "\n",
    "  return fname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKPL94L6WJtI"
   },
   "source": [
    "## **Feature Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQp8fthJWWHI"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------  Feature Selection - Gain Ratio\n",
    "\n",
    "def FS_GainRatio (fname, num_features=1):\n",
    "\n",
    "\n",
    "  \n",
    "  gain_ratio_scores = {}\n",
    "  gr_score = pd.DataFrame(columns = ['Features', 'Gain Ratio', 'Dataset'])\n",
    "\n",
    "  # -----------------------------------------  Reading dataset\n",
    "  Dataset = pd.read_csv(fname)\n",
    "  ind = Dataset.iloc[:, 0:-1]\n",
    "  dep = Dataset.iloc[:,-1]\n",
    "    \n",
    "  logger.info(f\"Total number of features before FS: {ind.shape[1]}\")\n",
    "    \n",
    "  # -----------------------------------------  GR for Each Independent Var\n",
    "  for col in ind.columns:\n",
    "    score = info_gain.info_gain_ratio(dep, ind[col])\n",
    "    gain_ratio_scores[col] = score\n",
    "    gr_score = pd.concat([gr_score, pd.DataFrame([{'Features' : col, 'Gain Ratio' : score, 'Dataset' : getDatasetName(fname)}])], ignore_index = True )\n",
    "\n",
    "  # -----------------------------------------  Sort Features (GR)\n",
    "  #best = sorted(gain_ratio_scores.items(), key=lambda x: x[1], reverse=True)[0:num_features]\n",
    "  best = sorted(gain_ratio_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "  # -----------------------------------------  Calculate Score Threshold (Mean)\n",
    "  mean_score = mean([b[1] for b in best])\n",
    "  gr_score['Threshold']= mean_score\n",
    "\n",
    "  # -----------------------------------------  Filter Features - Threshold\n",
    "  best_features = [b[0] for b in best if b[1] >= mean_score]\n",
    "  best_scores   = [b[1] for b in best if b[1] >= mean_score]\n",
    "\n",
    "  # -----------------------------------------  Save GR results\n",
    "  Save_CSV (gr_score, fname.replace('.csv','_GainRatioValues.csv'))\n",
    "\n",
    "  # -----------------------------------------  Save reduced file\n",
    "  fname = fname.replace('.csv','_GainRatio.csv')\n",
    "  ReducedDataset = pd.concat([ind[best_features], dep], axis=1, sort=False)\n",
    "  ReducedDataset.to_csv(fname, \",\", index = False)\n",
    "  \n",
    "  logger.info(f\"Total number of features after FS: {ReducedDataset.shape[1] -1}\")\n",
    "\n",
    "  return fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FS_GainRatio(fname, num_features=1):\n",
    "    #gain_ratio_scores = {}\n",
    "    #gr_score = pd.DataFrame(columns = ['Features', 'Gain Ratio', 'Dataset'])\n",
    "  \n",
    "    # Read dataset\n",
    "    Dataset = pd.read_csv(fname)\n",
    "    ind = Dataset.iloc[:, :-1]\n",
    "    dep = Dataset.iloc[:, -1]\n",
    "    \n",
    "    logger.info(f\"Total number of features before FS: {ind.shape[1]}\")\n",
    "    \n",
    "    # Calculate Gain Ratio for each feature\n",
    "    gain_ratio_scores = {col: info_gain.info_gain_ratio(dep, ind[col]) for col in ind.columns}\n",
    "    \n",
    "    # Create DataFrame from scores\n",
    "    gr_score = pd.DataFrame([\n",
    "        {'Features': col, 'Gain Ratio': score, 'Dataset': getDatasetName(fname)}\n",
    "        for col, score in gain_ratio_scores.items()\n",
    "    ])\n",
    "\n",
    "    # Sort features by Gain Ratio\n",
    "    best = sorted(gain_ratio_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Calculate mean score as threshold\n",
    "    mean_score = mean(score for _, score in best)\n",
    "    gr_score['Threshold'] = mean_score\n",
    "\n",
    "    # Filter features by the threshold\n",
    "    best_features = [feature for feature, score in best if score >= mean_score]\n",
    "\n",
    "    # Save Gain Ratio results\n",
    "    Save_CSV(gr_score, fname.replace('.csv', '_GainRatioValues.csv'))\n",
    "\n",
    "    # Save reduced dataset\n",
    "    fname = fname.replace('.csv', '_GainRatio.csv')\n",
    "    ReducedDataset = pd.concat([ind[best_features], dep], axis=1)\n",
    "    ReducedDataset.to_csv(fname, index=False)\n",
    "  \n",
    "    logger.info(f\"Total number of features after FS: {len(best_features)}\")\n",
    "\n",
    "    return fname\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xGd8XWC3IuMv"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------  Feature Selection - Principal Component Analysis\n",
    "\n",
    "def FS_PCA (fname, pca_num=2):\n",
    "\n",
    "  ind, dep, _, _ = prepareVars (fname)\n",
    "  logger.info(f\"Total number of features before FS: {ind.shape[1]}\")\n",
    "    \n",
    "  fname = fname.replace('.csv','_PCA.csv')\n",
    "\n",
    "  pca = PCA(n_components=pca_num)\n",
    "  com = pca.fit_transform(ind)\n",
    "\n",
    "  ReducedDataset = pd.concat([pd.DataFrame(data=com), dep], axis=1, sort=False)\n",
    "  ReducedDataset.to_csv(fname, ',', index = False)\n",
    "  logger.info(f\"Total number of features after FS: {ReducedDataset.shape[1] -1}\")\n",
    "    \n",
    "  if _WriteFlag :\n",
    "    output = getDatasetName (fname) + ' : ' + str(pca.explained_variance_ratio_) + '\\n'\n",
    "    Save_File (output, fname.replace('.csv','_Variance.txt'))\n",
    "\n",
    "  Remove_Unsupp_Clfs ()\n",
    "  PCA_variance_ratio_plot(pca)\n",
    "\n",
    "  return fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TaDdhFlgQ0hU"
   },
   "outputs": [],
   "source": [
    "# ------------------------- Remove Naive Bayes classifiers - when using PCA\n",
    "def Remove_Unsupp_Clfs ():\n",
    "    try:\n",
    "        #indClfs.pop('NB(B)')\n",
    "        #indClfs.pop('NB(G)')\n",
    "        #indClfs.pop('NB(M)')\n",
    "        keys_to_remove = [key for key in indClfs.keys() if 'NB(M)' in key]\n",
    "        # Remove the keys from the dictionary\n",
    "        for key in keys_to_remove:\n",
    "            indClfs.pop(key, None) \n",
    "    \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_variance_ratio_plot(pca):\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    y = np.cumsum(pca.explained_variance_ratio_)\n",
    "    xi = np.arange(1, len(y)+1 , step=1)\n",
    "    \n",
    "    plt.ylim(0.0,1.1)\n",
    "    plt.plot(xi, y, marker='o', linestyle='--', color='b')\n",
    "    \n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.xticks(np.arange(0, len(y)+1, step=1)) #change from 0-based array index to 1-based human-readable label\n",
    "    plt.ylabel('Cumulative variance (%)')\n",
    "    plt.title('The number of components needed to explain variance')\n",
    "    \n",
    "    plt.axhline(y=0.95, color='r', linestyle='-')\n",
    "    plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n",
    "    \n",
    "    ax.grid(axis='x')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFb0LKJIKcjJ"
   },
   "source": [
    "# **Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "awIUZhwnDMif"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------  Plot BarChart\n",
    "\n",
    "def Plot_BarChart (allRes, fname, Color=None, Hue = None):\n",
    "\n",
    "  metrices = barMetric\n",
    "\n",
    "  for metric in metrices:\n",
    "\n",
    "    f, ax = plt.subplots (figsize=(8, 8))\n",
    "\n",
    "    sns.set_color_codes ('muted')\n",
    "    fig = sns.barplot (x=metric, y='Classifier', hue=Hue, palette='Set2', data=allRes, color=Color)\n",
    "    sns.despine()\n",
    "\n",
    "    plt.xlabel (metric+ ' %')\n",
    "    plt.title (getDatasetName(fname), fontweight='bold',size=12)\n",
    "\n",
    "    if _WriteFlag : fig.get_figure().savefig(fname.replace('_results.csv','') + '_' + metric + '.png' , bbox_inches = 'tight', dpi=dpi)\n",
    "\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWaZzIgRLMtX"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------  Plot Boxplot\n",
    "\n",
    "def Plot_Boxplot (boxRes, fname, Color = None, Hue = None):\n",
    "\n",
    "  metrices = boxMetrics\n",
    "\n",
    "  for metric in metrices: \n",
    "\n",
    "    f, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    flierprops = dict(markerfacecolor='0.75', markersize=5, linestyle='none')\n",
    "\n",
    "    sns.boxplot (x=metric, y='Classifier', data=boxRes, hue=Hue, palette='vlag', flierprops=flierprops) #palette=\"vlag\"\n",
    "\n",
    "    plt.ylabel ('Classifier', size=12)\n",
    "    plt.xlabel (metric, size=12)\n",
    "    plt.title (getDatasetName(fname), fontweight='bold',size=12)\n",
    "\n",
    "    if _WriteFlag : plt.savefig(fname.replace('_results.csv', '_' + metric + '_boxplot') + \".png\" , bbox_inches = 'tight', dpi=dpi )\n",
    "\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjxMObsaM-2o"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------  Plot ROC Curve Plot\n",
    "\n",
    "def Plot_ROC (rocRes, fname):\n",
    "\n",
    "  fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "  for i in rocRes.index:\n",
    "    plt.plot ( rocRes.loc[i]['FPR'],\n",
    "               rocRes.loc[i]['TPR'],\n",
    "               label='%s' % (i) + ' (' + str(round(rocRes.loc[i]['AUC'],2)) + ')')\n",
    "\n",
    "  plt.plot ([0,1], [0,1], color='black', linestyle='--')\n",
    "\n",
    "  plt.xticks (np.arange(0.0, 1.1, step=0.1))\n",
    "  plt.xlabel ('False Positive Rate', fontsize=12)\n",
    "\n",
    "  plt.yticks (np.arange(0.0, 1.1, step=0.1))\n",
    "  plt.ylabel ('True Positive Rate', fontsize=12)\n",
    "\n",
    "  plt.title (getDatasetName(fname), fontweight='bold', fontsize=13)\n",
    "  plt.legend (prop={'size':13}, loc='lower right', bbox_to_anchor=(1.37, 0))\n",
    "\n",
    "  if _WriteFlag : fig.savefig(fname.replace('_results.csv','_roc_curve') + '.png' , bbox_inches = 'tight', dpi=dpi)\n",
    "\n",
    "  #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubSav6RYsfnH"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------  Plot ScottKnottESD\n",
    "\n",
    "def Plot_ScottKnottESD (boxplot_rank, fname):\n",
    "    \n",
    "  boxplot_rank['rank'] = boxplot_rank['rank'].astype('category')\n",
    "\n",
    "  flip_xlabels = theme(\n",
    "      axis_text_x = element_text(size=8,angle = 30, hjust = 6, color=\"black\"),\n",
    "      panel_background = element_rect(fill='white', alpha=.2),\n",
    "      panel_border = element_rect(color='gray', size=1),\n",
    "      legend_position = 'none',\n",
    "      plot_title = element_text(size = 10, face = \"bold\"),\n",
    "      axis_title_x = element_text(size=10, color=\"black\"),\n",
    "      axis_title_y = element_text(size=10, color=\"black\")\n",
    "    )\n",
    "\n",
    "  ncol = boxplot_rank['rank'].nunique()\n",
    "  #  palette= BuPu or Blues or YlGnBu, (type='div', palette='RdYlBu')\n",
    "  img = ggplot(boxplot_rank, aes(x='Classifier', y='AUC', fill='rank')) + \\\n",
    "      geom_boxplot(alpha=.8) + \\\n",
    "      facet_wrap(['rank'], ncol = ncol, scales='free_x') + \\\n",
    "      scale_fill_brewer(palette='BuPu') + \\\n",
    "      flip_xlabels + \\\n",
    "      ggtitle(getDatasetName (fname))\n",
    "\n",
    "  print (img)\n",
    "    \n",
    "  #ggsave(plot=img, filename=fname.replace('_results.csv','_ESD') + '.png', dpi=dpi)\n",
    "  if _WriteFlag : img.save(filename=fname.replace('_results.csv','_ESD') + '.png', dpi=dpi) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHXmzzsQJWRa"
   },
   "source": [
    "#  **Model Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JJSc306H3_R"
   },
   "source": [
    "## **Cross Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ZRE0x5gVaOl"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------  Cross Validation Type\n",
    "\n",
    "def Cross_Fold ():\n",
    "\n",
    "  if   CV == '10SCV' : folds = StratifiedKFold(n_splits=10)\n",
    "  elif CV == '10'    : folds = KFold(n_splits=10, random_state = 42)\n",
    "  elif CV == 'LOO'   : folds = LeaveOneOut()\n",
    "  elif CV == '5x2'   : folds = RepeatedStratifiedKFold(n_splits=2,  n_repeats=5,  random_state=1)\n",
    "  elif CV == '10x10' : folds = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n",
    "\n",
    "  return folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GDb2tpurstu"
   },
   "source": [
    "## **Evaluate Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcc(tp,fp,tn,fn):\n",
    "    try:\n",
    "        return (tp*tn - fp*fn)/((tp+fp)*(tp+fn)*(fn+tn)*(fp+tn))**0.5\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nsdKSh6jEqTU"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------  Fun: Evaluate ML Models\n",
    "\n",
    "def Evaluate_Model (clf, algorithm_name, ind, dep, fname, type = 'Individual'):\n",
    "  global Res, Roc, Det\n",
    "\n",
    "  folds = Cross_Fold ()\n",
    "\n",
    "  scoring = {\n",
    "      'Accuracy'  : 'accuracy',\n",
    "      'Precision' : 'precision',\n",
    "      'Recall'    : 'recall',\n",
    "      'F1-score'  : 'f1',\n",
    "      'F1-micro'  : 'f1_micro',\n",
    "      'F1-weight' : 'f1_weighted',\n",
    "      'Brier'     : 'neg_brier_score',\n",
    "      'AUC'       : 'roc_auc',\n",
    "      'MCC'       : make_scorer(matthews_corrcoef),\n",
    "      'Confusion' : make_scorer(ConfusionMatrix, greater_is_better=False, algorithm_name=algorithm_name, fname=fname, type=type)\n",
    "  }\n",
    "\n",
    "  print (clf)\n",
    "\n",
    "  scores = cross_validate (clf, ind, dep, cv=folds, scoring=scoring, n_jobs=-1)\n",
    "\n",
    "  acc_results = scores ['test_Accuracy']\n",
    "  pre_results = scores ['test_Precision']\n",
    "  rec_results = scores ['test_Recall']\n",
    "  f1s_results = scores ['test_F1-score']\n",
    "  f1m_results = scores ['test_F1-micro']\n",
    "  f1w_results = scores ['test_F1-weight']\n",
    "  bre_results = abs( scores ['test_Brier'] )\n",
    "  auc_results = scores ['test_AUC']\n",
    "  mcc_results = scores ['test_MCC']\n",
    "    \n",
    "  Res = pd.concat([Res, pd.DataFrame([{'Classifier' : algorithm_name,\n",
    "                    'Accuracy'   : np.round(acc_results.mean() * 100,2),\n",
    "                    'Precision'  : np.round(pre_results.mean() * 100,2),\n",
    "                    'Recall'     : np.round(rec_results.mean() * 100,2),\n",
    "                    'F1-score'   : np.round(f1s_results.mean() * 100,2),\n",
    "                    'F1-micro'   : np.round(f1m_results.mean() * 100,2),\n",
    "                    'F1-weight'  : np.round(f1w_results.mean() * 100,2),\n",
    "                    'Brier'      : np.round(abs(bre_results.mean()),2),\n",
    "                    'AUC'        : np.round(auc_results.mean(),2),\n",
    "                    'MCC'        : np.round(mcc_results.mean(), 2), \n",
    "                    'Type'       : type,\n",
    "                    'Dataset'    : getDatasetName (fname)}])], ignore_index=True)\n",
    "\n",
    "  for i in range(0,len(acc_results)):\n",
    "    Det =  pd.concat([Det, pd.DataFrame([{'Classifier' : algorithm_name,\n",
    "                      'Accuracy'   : np.round(acc_results[i] * 100,2),\n",
    "                      'Precision'  : np.round(pre_results[i] * 100,2),\n",
    "                      'Recall'     : np.round(rec_results[i] * 100,2),\n",
    "                      'F1-score'   : np.round(f1s_results[i] * 100,2),\n",
    "                      'F1-micro'   : np.round(f1m_results[i] * 100,2),\n",
    "                      'F1-weight'  : np.round(f1w_results[i] * 100,2),\n",
    "                      'Brier'      : np.round(abs(bre_results[i]),2),\n",
    "                      'AUC'        : np.round(auc_results[i],2),\n",
    "                      'MCC'        : np.round(mcc_results.mean(), 2), \n",
    "                      'Type'       : type,\n",
    "                      'Dataset'    : getDatasetName (fname)}])],\n",
    "                      ignore_index = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MLteKVXyTbq4"
   },
   "outputs": [],
   "source": [
    "def ConfusionMatrix (y_true, y_pred, algorithm_name, fname, type, **kwargs):\n",
    "  global Mat\n",
    "\n",
    "  y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "\n",
    "  for i in range(0,len(y_true)):\n",
    "    Mat =  pd.concat([Mat, pd.DataFrame([{'Classifier' : algorithm_name,\n",
    "                      'Actual'     : y_true[i],\n",
    "                      'Pred'       : y_pred[i],\n",
    "                      'Type'       : type,\n",
    "                      'Dataset'    : getDatasetName (fname)}])],\n",
    "                      ignore_index=True)\n",
    "\n",
    "  return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gN6u-t4S0u1W"
   },
   "source": [
    "## **Hyperparameters Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOjS69bBwpOw"
   },
   "outputs": [],
   "source": [
    "def Tune_Model (clf, algorithm_name, ind, dep, fname):\n",
    "\n",
    "  TuneDF = prepareTuneDataFrame ()\n",
    "\n",
    "  print( colors.BOLD + colors.RED + '======================   Hyperparameters Tuning ('+algorithm_name+'). Be patient ðŸ˜Š')\n",
    "\n",
    "  folds = RepeatedStratifiedKFold(n_splits=2, n_repeats=5, random_state=1)\n",
    "  # folds = StratifiedKFold(n_splits=5, random_state=1)\n",
    "\n",
    "  space = dict()\n",
    "\n",
    "  if algorithm_name == 'DT':\n",
    "    space = {\n",
    "        'max_depth'        : [None, 2, 4, 6, 8, 10, 12],\n",
    "        'splitter'         : ['best', 'random'],\n",
    "        'max_features'     : [None, 'auto', 'sqrt', 'log2'],\n",
    "        'criterion'        : ['gini', 'entropy']\n",
    "    }\n",
    "\n",
    "  elif algorithm_name == 'LR':\n",
    "    space = {\n",
    "        'solver'  : ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "        'penalty' : ['none', 'l1', 'l2', 'elasticnet'],\n",
    "        'C'       : [0.01, 0.1, 1, 10, 100]\n",
    "    }\n",
    "\n",
    "  elif algorithm_name == 'MLP':\n",
    "    space = {\n",
    "        'hidden_layer_sizes' : [(100,)],\n",
    "        'activation'         : ['relu', 'identity', 'logistic','tanh'],\n",
    "        'solver'             : ['adam', 'lbfgs', 'sgd'],\n",
    "        'alpha'              : [0.0001, 0.05],\n",
    "        'learning_rate'      : ['constant', 'invscaling', 'adaptive']\n",
    "    }\n",
    "\n",
    "  elif algorithm_name == 'SGD':\n",
    "    space = {\n",
    "        'loss'          : ['squared_loss', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "        'penalty'       : ['l1', 'l2', 'elasticnet'],\n",
    "        'alpha'         : [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "        'learning_rate' : ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "        'class_weight'  : [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}],\n",
    "        'eta0'          : [0.01, 1, 10, 100]\n",
    "    }\n",
    "\n",
    "  elif algorithm_name == 'GP':\n",
    "    space = {\n",
    "        'kernel' : [None, 1.0*RBF(1.0), 1.0*Matern(1.0)]\n",
    "    }\n",
    "\n",
    "  elif algorithm_name == 'LDA':\n",
    "    space = {\n",
    "        'solver' : ['svd', 'lsqr', 'eigen'],\n",
    "        'tol'    : [0.0001,0.0002,0.0003]\n",
    "    }\n",
    "\n",
    "  elif algorithm_name == 'KNN':\n",
    "    space = {\n",
    "        'n_neighbors' : range(1, 10, 2),\n",
    "        'weights'     : ['uniform', 'distance'],\n",
    "        'metric'      : ['euclidean', 'manhattan', 'minkowski'],\n",
    "        'algorithm'   : ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "    }\n",
    "\n",
    "  elif algorithm_name == 'SVM':\n",
    "    space = {\n",
    "        'kernel' : ['rbf', 'linear', 'poly', 'sigmoid'],\n",
    "        'C'      : [1.0, 100, 10, 0.1, 0.001],\n",
    "        'gamma'  : ['scale', 'auto']\n",
    "    }\n",
    "\n",
    "  elif 'BG-' in algorithm_name:\n",
    "    space = {\n",
    "        'n_estimators' : [10, 100, 1000],\n",
    "        'bootstrap'    : [True, False]\n",
    "    }\n",
    "\n",
    "  elif  algorithm_name in  ['BS-','Ada']:\n",
    "    space = {\n",
    "        'n_estimators'  : [50, 100, 1000],\n",
    "        'learning_rate' : [1, 0.1, 0.001, 0.01]\n",
    "    }\n",
    "\n",
    "  elif 'Stack' in algorithm_name:\n",
    "    space = {\n",
    "    }\n",
    "\n",
    "  elif 'Vote' in algorithm_name:\n",
    "    space = {\n",
    "    }\n",
    "\n",
    "  elif algorithm_name == 'RF':\n",
    "    space = {\n",
    "        # 'n_estimators'      : [100, 50, 500, 1000],\n",
    "        'n_estimators'      : [100, 50, 40, 30],\n",
    "        # 'max_features'      : ['auto', 'sqrt', 'log2'],\n",
    "        # 'min_samples_split' : [2, 5, 10],\n",
    "        # 'min_samples_leaf'  : [1, 2, 4],\n",
    "        'criterion'         : ['gini', 'entropy'],\n",
    "        # 'bootstrap'         : [True, False],\n",
    "        'max_depth'           : [1, 4, 8],\n",
    "        'min_samples_leaf'    : [1, 10, 5]\n",
    "\n",
    "    }\n",
    "\n",
    "  elif algorithm_name == 'ET':\n",
    "    space = {\n",
    "        # 'n_estimators'      : [100, 50, 500, 1000],\n",
    "        'n_estimators'      : [100, 50, 40, 30],\n",
    "        #'max_features'      : ['auto', 'sqrt', 'log2'],\n",
    "        #'min_samples_split' : [2, 5, 10],\n",
    "        'min_samples_leaf'  : [1, 2, 4],\n",
    "        'criterion'         : ['gini', 'entropy'],\n",
    "        #'bootstrap'         : [False, True],\n",
    "        'max_depth'           : [1, 4, 8]\n",
    "    }\n",
    "\n",
    "  elif algorithm_name == 'GB':\n",
    "    space = {\n",
    "        'n_estimators'      : [100, 50, 500, 1000],\n",
    "        'loss'              : ['deviance', 'exponential'],\n",
    "        'learning_rate'     : [0.1, 0.001, 0.01],\n",
    "        # 'subsample'       : [1.0, 0.5, 0.7 ],\n",
    "        'max_depth'         : [3, 7, 9],\n",
    "        'min_samples_leaf'  : [1, 10, 5]\n",
    "    }\n",
    "\n",
    "  elif algorithm_name == 'HGB':\n",
    "    space = {\n",
    "        'loss'              : ['auto', 'binary_crossentropy', 'categorical_crossentropy'],\n",
    "        'max_iter'          : [100, 50, 500, 1000],\n",
    "        'learning_rate'     : [0.1, 0.001, 0.01],\n",
    "        'max_depth'         : [None, 1, 3, 5],\n",
    "        'min_samples_leaf'  : [20, 10, 5],\n",
    "        # 'max_depth'         : [None, 25, 50, 75],\n",
    "        # 'l2_regularization' : [0, 1.5]\n",
    "    }\n",
    "\n",
    "  elif algorithm_name == 'XGB':\n",
    "    space = {\n",
    "        'n_estimators'     : [100, 50, 500, 1000],\n",
    "        'learning_rate'    : [0.3, 0.1, 0.001, 0.01],\n",
    "        'max_depth'        : [6, 3, 4, 5],\n",
    "        # 'subsample'        : [1.0, 0.6, 0.8],\n",
    "        # 'colsample_bytree' : [1.0, 0.6, 0.8],\n",
    "        # 'min_child_weight' : [1, 5, 10],\n",
    "        # 'gamma'            : [0, 0.5, 1, 1.5, 2, 5]\n",
    "    }\n",
    "\n",
    "  elif algorithm_name == 'CAT':\n",
    "    space = {\n",
    "        'n_estimators'      : [1000, 100, 50, 500],\n",
    "        'depth'             : [3, 1 , 5, 10],\n",
    "        'learning_rate'     : [0.03, 0.001, 0.01, 0.1],\n",
    "        'loss_function'     : ['Logloss', 'MultiClass'],\n",
    "        'min_data_in_leaf'  : [1, 10],\n",
    "        # 'l2_leaf_reg'       : [1, 10, 100]\n",
    "    }\n",
    "\n",
    "  start_time = time.time()\n",
    "\n",
    "  search = GridSearchCV (clf, space, n_jobs=-1, cv=folds, scoring = 'roc_auc')\n",
    "  result = search.fit (ind, dep)\n",
    "\n",
    "  tune_time = round((time.time() - start_time))\n",
    "\n",
    "  if _WriteFlag:\n",
    "    TuneDF = pd.concat([TuneDF, pd.DataFrame([{'Dataset'    : getDatasetName(fname),\n",
    "                            'Classifier' : algorithm_name,\n",
    "                            'Time'       : tune_time,\n",
    "                            'Params'     : result.best_estimator_.get_params()}])]\n",
    "                           ,ignore_index=True)\n",
    "\n",
    "    Save_CSV (TuneDF, '_HyperTuning.csv', 'a')\n",
    "\n",
    "  return result.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXvMzc3nV7pc"
   },
   "source": [
    "# **Machine Learning Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIEG5NCNCgNh"
   },
   "source": [
    "## **Prepare Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_selection_to_names(clfs):\n",
    "\n",
    "    if _PCA:\n",
    "        # Create a new dictionary with 'PCA' appended to the keys if feature selection is True\n",
    "        clfs = {f\"{name}-PCA\": clf for name, clf in clfs.items()}\n",
    "    elif _GainRatio:\n",
    "        # Create a new dictionary with 'PCA' appended to the keys if feature selection is True\n",
    "        clfs = {f\"{name}-GR\": clf for name, clf in clfs.items()}\n",
    "        \n",
    "    return clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WcYOPaKLBpeq"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------  Prepare Classifiers\n",
    "\n",
    "def prepareIndividualModels ():\n",
    "\n",
    "  clfs = {\n",
    "            'DT'   : DecisionTreeClassifier(),\n",
    "            'LR'   : LogisticRegression(),\n",
    "            'SVM'  : CalibratedClassifierCV(LinearSVC()) ,  #SVC(kernel='linear', max_iter=1000, probability = True), #SVC(kernel='linear', max_iter=1000, probability = True)\n",
    "            'MLP'  : MLPClassifier(),\n",
    "            'SGD'  : SGDClassifier(loss= 'log_loss'),\n",
    "            #'GP'   : GaussianProcessClassifier(),\n",
    "            'NB(B)': BernoulliNB(),\n",
    "            'NB(G)': GaussianNB(),\n",
    "            'NB(M)': MultinomialNB(),\n",
    "            #'LDA'  : LinearDiscriminantAnalysis(),\n",
    "            'KNN'  : KNeighborsClassifier()\n",
    "         }\n",
    "\n",
    "  clfs = add_feature_selection_to_names(clfs)\n",
    "\n",
    "  return clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8IcNxBdnn0we"
   },
   "outputs": [],
   "source": [
    "def prepareBaggingEnsemble ():\n",
    "\n",
    "  clfs = indClfs.copy()\n",
    "\n",
    "  for name in indClfs:\n",
    "\n",
    "    bagName = 'BG-' + name\n",
    "    clfs[bagName] = clfs.pop(name)\n",
    "\n",
    "  return clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GMuKezh5OYA8"
   },
   "outputs": [],
   "source": [
    "def prepareBoostingEnsemble ():\n",
    "\n",
    "  clfs = indClfs.copy()\n",
    "\n",
    "  for name in indClfs:\n",
    "\n",
    "    # ------------- Remove clfs that doesn't support sample_weight\n",
    "    if name in {'MLP','GP','KNN','LDA'}:\n",
    "      clfs.pop (name)\n",
    "      continue\n",
    "\n",
    "    bosName = 'BS-' + name\n",
    "    clfs[bosName] = clfs.pop(name)\n",
    "\n",
    "  return clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sWJ9nn72aWx6"
   },
   "outputs": [],
   "source": [
    "def prepareTreeBasedEnsembles ():\n",
    "\n",
    "  clfs = {\n",
    "      'Ada' : AdaBoostClassifier (algorithm=\"SAMME\"),\n",
    "      'RF'  : RandomForestClassifier (random_state=0),\n",
    "      'ET'  : ExtraTreesClassifier (random_state=0),\n",
    "      'GB'  : GradientBoostingClassifier (random_state=0),\n",
    "      'HGB' : HistGradientBoostingClassifier (),\n",
    "      'XGB' : XGBClassifier (verbose=0),\n",
    "      'CAT' : CatBoostClassifier (verbose=0)\n",
    "  }\n",
    "    \n",
    "  clfs = add_feature_selection_to_names(clfs)\n",
    "    \n",
    "  return clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stacking_meta ():\n",
    "    m = {\n",
    "      'LR'  : LogisticRegression(),\n",
    "      'DT'  : DecisionTreeClassifier(),\n",
    "      #'SVM' : SVC(kernel='linear', probability = True)\n",
    "      }\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPdE54O0lAr9"
   },
   "outputs": [],
   "source": [
    "def prepareStackClassifier ():\n",
    "\n",
    "  clfs = []\n",
    "  stacks = {}\n",
    "\n",
    "  if incInd :\n",
    "    for key, value in indClfs.items():\n",
    "      temp = [key,value]\n",
    "      clfs.append(temp)\n",
    "\n",
    "  elif incTen :\n",
    "    for key, value in tenClfs.items():\n",
    "      temp = [key,value]\n",
    "      clfs.append(temp)\n",
    "\n",
    "  meta = get_stacking_meta ()\n",
    "\n",
    "  for name in meta:\n",
    "    staName = 'Stack' + '-' + name\n",
    "    stacks[staName] = clfs\n",
    "      \n",
    "  stacks = add_feature_selection_to_names(stacks)\n",
    "    \n",
    "  return stacks, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPdE54O0lAr9"
   },
   "outputs": [],
   "source": [
    "def prepareTreeStackClassifier ():\n",
    "\n",
    "  clfs = []\n",
    "  stacks = {}\n",
    "\n",
    "  for key, value in tenClfs.items():\n",
    "      temp = [key,value]\n",
    "      clfs.append(temp)\n",
    "\n",
    "  meta = get_stacking_meta ()\n",
    "\n",
    "  for name in meta:\n",
    "    staName = 'TBStack' + '-' + name\n",
    "    stacks[staName] = clfs\n",
    "\n",
    "  stacks = add_feature_selection_to_names(stacks)\n",
    "    \n",
    "  return stacks, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZaYqZPmeYOKc"
   },
   "outputs": [],
   "source": [
    "def prepareVotingEnsemble ():\n",
    "\n",
    "  clfs = []\n",
    "  vote = {}\n",
    "\n",
    "  for key, value in indClfs.items():\n",
    "    temp = [key,value]\n",
    "    clfs.append(temp)\n",
    "\n",
    "  vote['Vote'] = clfs\n",
    "  vote = add_feature_selection_to_names(vote)\n",
    "    \n",
    "  return vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RAO26k8w_bos"
   },
   "outputs": [],
   "source": [
    "def prepareModelsTuning (clfs, ind, dep, fname, type='Individual'):\n",
    "  global indClfs, unIndClfs, tenClfs, unTenClfs\n",
    "\n",
    "  for name in clfs:\n",
    "\n",
    "    clf = clfs [name]\n",
    "    clf = Tune_Model (clf, name, ind, dep, fname)\n",
    "\n",
    "    if type == 'Individual':\n",
    "      unIndClfs ['D-' + name] =  indClfs [name]\n",
    "      indClfs [name] = clf\n",
    "\n",
    "    if type == 'TreeBased':\n",
    "      unTenClfs ['D-' + name] =  tenClfs [name]\n",
    "      tenClfs [name] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8HMck5Y0TtZ"
   },
   "outputs": [],
   "source": [
    "def Create_Ensemble (clf, type, meta = ''):\n",
    "\n",
    "  if   type == 'Bagging'      :  clf = BaggingClassifier  (base_estimator=clf)\n",
    "  elif type == 'Boosting'     :  clf = AdaBoostClassifier (base_estimator=clf, algorithm=\"SAMME\")\n",
    "  elif type == 'Stacking'     :  clf = StackingClassifier (estimators=clf, final_estimator=meta)\n",
    "  elif type == 'TreeStacking' :  clf = StackingClassifier (estimators=clf, final_estimator=meta)\n",
    "  elif type == 'Voting'       :  clf = VotingClassifier   (estimators=clf, voting='soft')\n",
    "\n",
    "  return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Wv0nPKDhZoa"
   },
   "source": [
    "## **Prepare Dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eOda0HR9IKcQ"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------  Prepare Data Frames\n",
    "\n",
    "def prepareDataFrames ():\n",
    "\n",
    "  ResDF  = pd.DataFrame  (columns = ['Classifier', 'Accuracy', 'Precision', 'Recall', 'F1-score','F1-micro', 'F1-weight', 'Brier', 'AUC', 'MCC', 'Type', 'Dataset'])\n",
    "  RocDF  = pd.DataFrame  (columns = ['Classifier', 'FPR', 'TPR', 'AUC', 'Type', 'Dataset'])\n",
    "  DetDF  = pd.DataFrame  (columns = ['Classifier', 'Accuracy', 'Precision', 'Recall', 'F1-score','F1-micro', 'F1-weight', 'Brier', 'AUC', 'MCC', 'Type', 'Dataset'])\n",
    "  MatDF  = pd.DataFrame  (columns = ['Classifier', 'Actual', 'Pred', 'Type', 'Dataset'])\n",
    "  TimeDF = pd.DataFrame  (columns = ['Classifier', 'Dataset', 'Training Time'])\n",
    "\n",
    "  return ResDF, RocDF, DetDF, MatDF, TimeDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XxcIpoT1AnA-"
   },
   "outputs": [],
   "source": [
    "def prepareTuneDataFrame ():\n",
    "\n",
    "  TuneDF = pd.DataFrame  (columns = ['Dataset', 'Classifier', 'Time', 'Params'])\n",
    "\n",
    "  return TuneDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fotwwPteV9FR"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------  Prepare Stat Data Frames\n",
    "\n",
    "def prepareStatDataFrame ():\n",
    "\n",
    "  StatDF = pd.DataFrame (columns = ['Classifier_1', 'Classifier_2', 'Test', 'Stat', 'p-value', 'Null Hypo', 'Win', 'Lost', 'Dataset'])\n",
    "\n",
    "  return StatDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2UxjiQHCqeN"
   },
   "source": [
    "## **Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HbBXkflLwTTM"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------  Machine Learning Models\n",
    "\n",
    "def Machine_Learning_Models (clfs, ind, dep, fname, type = 'Individual', meta = ''):\n",
    "  global Res, Roc, Det, Mat, TimeDF\n",
    "\n",
    "  Res, Roc, Det, Mat, TimeDF = prepareDataFrames ()\n",
    "\n",
    "  fname = getPath(type) + fname.replace('.csv','_results.csv')\n",
    "\n",
    "  if type in ['Stacking', 'TreeStacking']:\n",
    "\n",
    "    for (name,clf), (m,mClf) in zip(clfs.items(), meta.items()):\n",
    "\n",
    "      clf = Create_Ensemble (clf, type, mClf)\n",
    "      # log training time\n",
    "      start_time = time.time()\n",
    "      Evaluate_Model (clf, name, ind, dep, fname, type)\n",
    "      logger.info(f\"Training and evaluation time for {name}: {time.time() - start_time:.3f} seconds\")\n",
    "      TimeDF = pd.concat([TimeDF, pd.DataFrame([{'Classifier' : name,\n",
    "                    'Dataset'    : getDatasetName (fname),\n",
    "                    'Training Time'  : round(time.time() - start_time, 3),\n",
    "                    'Type'       : type,\n",
    "                    }])], ignore_index=True)\n",
    "\n",
    "  else:\n",
    "    for ( name , clf ) in clfs.items():\n",
    "\n",
    "      if   type == 'Bagging'  : clf = Create_Ensemble (clf, type)\n",
    "      elif type == 'Boosting' : clf = Create_Ensemble (clf, type)\n",
    "      elif type == 'Voting'   : clf = Create_Ensemble (clf, type)\n",
    "      # log time\n",
    "      start_time = time.time()\n",
    "      Evaluate_Model (clf, name, ind, dep, fname, type)\n",
    "      logger.info(f\"Training and evaluation time for {name}: {time.time() - start_time:.3f} seconds\")\n",
    "      TimeDF = pd.concat([TimeDF, pd.DataFrame([{'Classifier' : name,\n",
    "                    'Dataset'    : getDatasetName (fname),\n",
    "                    'Training Time'  :round(time.time() - start_time, 3),\n",
    "                    'Type'       : type }])], ignore_index=True)\n",
    "\n",
    "  Roc.set_index ('Classifier', inplace=True)\n",
    "\n",
    "  Save_CSV (Res, fname)\n",
    "  Save_CSV (Det, fname.replace('results.csv', 'detailed_results.csv'))\n",
    "  Save_CSV (Mat, fname.replace('results.csv', 'matrix_results.csv'))\n",
    "  Save_CSV (TimeDF, fname.replace('results.csv', 'Training_Time.csv'))\n",
    "  \n",
    "\n",
    "  if _Bar : Plot_BarChart (Res, fname)\n",
    "  if _ROC : Plot_ROC      (Roc, fname)\n",
    "  if _Box : Plot_Boxplot  (Det, fname)\n",
    "  if _ESD : \n",
    "      try: Plot_ScottKnottESD (ScottKnottESD(Det, fname), fname)\n",
    "      except: print('Only one model cant run ESD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cf10grb8nS3d"
   },
   "source": [
    "## **Compare Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kUY_NIZ6JPiP"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------  Comparing Models Within Dataset\n",
    "\n",
    "def Compare_Models_Within_Dataset (ind, dep, indTun, depTun, fname):\n",
    "  global allRes, allRoc, allDet, allMat, allTimeDF, allStat, allDatasetRes, allDatasetRoc, allDatasetDet, allDatasetMat, allDatasetTimeDF, allDatasetStat\n",
    "\n",
    "  if _GridSearch:\n",
    "    if incInd : prepareModelsTuning (indClfs, indTun, depTun, fname, 'Individual')\n",
    "    if incTen : prepareModelsTuning (tenClfs, indTun, depTun, fname, 'TreeBased')\n",
    "\n",
    "  if incInd:\n",
    "    Machine_Learning_Models (indClfs, ind, dep, fname, 'Individual')\n",
    "    allRes = Res\n",
    "    allRoc = Roc\n",
    "    allDet = Det\n",
    "    allMat = Mat\n",
    "    allTimeDF = TimeDF\n",
    "    \n",
    "\n",
    "  if incBag:\n",
    "    bagClfs = prepareBaggingEnsemble ()\n",
    "    Machine_Learning_Models (bagClfs, ind, dep, fname, 'Bagging')\n",
    "    allRes  =  pd.concat([allRes, Res])\n",
    "    allRoc  =  pd.concat([allRoc, Roc])\n",
    "    allDet  =  pd.concat([allDet, Det])\n",
    "    allMat  =  pd.concat([allMat, Mat])\n",
    "    allTimeDF  =  pd.concat([allTimeDF, TimeDF])\n",
    "\n",
    "  if incBos:\n",
    "    bosClfs = prepareBoostingEnsemble ()\n",
    "    Machine_Learning_Models (bosClfs, ind, dep, fname, 'Boosting')\n",
    "    allRes  =  pd.concat([allRes, Res])\n",
    "    allRoc  =  pd.concat([allRoc, Roc])\n",
    "    allDet  =  pd.concat([allDet, Det])\n",
    "    allMat  =  pd.concat([allMat, Mat])\n",
    "    allTimeDF  =  pd.concat([allTimeDF, TimeDF])\n",
    "\n",
    "  if incTen:\n",
    "    Machine_Learning_Models (tenClfs, ind, dep, fname, 'TreeBased')\n",
    "    allRes  =  pd.concat([allRes, Res])\n",
    "    allRoc  =  pd.concat([allRoc, Roc])\n",
    "    allDet  =  pd.concat([allDet, Det])\n",
    "    allMat  =  pd.concat([allMat, Mat])\n",
    "      \n",
    "  if incVot:\n",
    "    votClfs = prepareVotingEnsemble ()\n",
    "    Machine_Learning_Models (votClfs, ind, dep, fname, 'Voting')\n",
    "    allRes  =  pd.concat([allRes, Res])\n",
    "    allRoc  =  pd.concat([allRoc, Roc])\n",
    "    allDet  =  pd.concat([allDet, Det])\n",
    "    allMat  =  pd.concat([allMat, Mat])\n",
    "    allTimeDF  =  pd.concat([allTimeDF, TimeDF])\n",
    "      \n",
    "  if incSta:\n",
    "    stackClfs, metaClfs = prepareStackClassifier ()\n",
    "    Machine_Learning_Models (stackClfs, ind, dep, fname, 'Stacking', metaClfs)\n",
    "    allRes  =  pd.concat([allRes, Res])\n",
    "    allRoc  =  pd.concat([allRoc, Roc])\n",
    "    allDet  =  pd.concat([allDet, Det])\n",
    "    allMat  =  pd.concat([allMat, Mat])\n",
    "    allTimeDF  =  pd.concat([allTimeDF, TimeDF])\n",
    "\n",
    "  if incTenSta:\n",
    "    tenstackClfs, metaClfs = prepareTreeStackClassifier ()\n",
    "    Machine_Learning_Models (tenstackClfs, ind, dep, fname, 'TreeStacking', metaClfs)\n",
    "    allRes  =  pd.concat([allRes, Res])\n",
    "    allRoc  =  pd.concat([allRoc, Roc])\n",
    "    allDet  =  pd.concat([allDet, Det])\n",
    "    allMat  =  pd.concat([allMat, Mat])\n",
    "    allTimeDF  =  pd.concat([allTimeDF, TimeDF])\n",
    "      \n",
    "\n",
    "  if _CompIndTune:\n",
    "    Machine_Learning_Models (unIndClfs, ind, dep, fname, 'Individual')\n",
    "    allRes  =  pd.concat([allRes, Res])\n",
    "    allRoc  =  pd.concat([allRoc, Roc])\n",
    "    allDet  =  pd.concat([allDet, Det])\n",
    "    allMat  =  pd.concat([allMat, Mat])\n",
    "    allTimeDF  =  pd.concat([allTimeDF, TimeDF])\n",
    "    Compare_Tune_Vs_Default (allRes, allRoc, allDet, fname)\n",
    "\n",
    "  if _CompTenTune:\n",
    "    Machine_Learning_Models (unTenClfs, ind, dep, fname, 'TreeBased' )\n",
    "    allRes  =  pd.concat([allRes, Res])\n",
    "    allRoc  =  pd.concat([allRoc, Roc])\n",
    "    allDet  =  pd.concat([allDet, Det])\n",
    "    allMat  =  pd.concat([allMat, Mat])\n",
    "    allTimeDF  =  pd.concat([allTimeDF, TimeDF])\n",
    "    Compare_Tune_Vs_Default (allRes, allRoc, allDet, fname)\n",
    "\n",
    "  if _CompInd:\n",
    "    Compare_Models_Vs_Indiv (allRes, allRoc, allDet, fname)\n",
    "\n",
    "  if _CompAll:\n",
    "    Compare_Models (allRes, allRoc, allDet, fname)\n",
    "    allDatasetRes  =  pd.concat([allDatasetRes, allRes])\n",
    "    allDatasetRoc  =  pd.concat([allDatasetRoc, allRoc])\n",
    "    allDatasetDet  =  pd.concat([allDatasetDet, allDet])\n",
    "    allDatasetMat  =  pd.concat([allDatasetMat, allMat])\n",
    "    allDatasetTimeDF  =  pd.concat([allDatasetTimeDF, allTimeDF])\n",
    "\n",
    "\n",
    "  if _StatFlag:\n",
    "    allStat = Stat_Test (allDet, fname, save=False)\n",
    "    allDatasetStat = pd.concat([allDatasetStat, allStat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qKhLXTT6nWb5"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------  Comparing Models\n",
    "\n",
    "def Compare_Models (allRes, allRoc, allDet, fname, type = ''):\n",
    "\n",
    "  fname = getPath('Compare') + fname.replace('.csv', '_' + type + '_results.csv')\n",
    "\n",
    "  if _ROC : Plot_ROC     (allRoc, fname)\n",
    "  if _Box : Plot_Boxplot (allDet, fname)\n",
    "  if _ESD : \n",
    "      try: Plot_ScottKnottESD (ScottKnottESD(allDet, fname), fname)\n",
    "      except: print('Cant run ESD')\n",
    "  \n",
    "\n",
    "  if _StatFlag : Stat_Test (allDet, fname.replace(getPath('Compare'), ''))\n",
    "\n",
    "  Save_CSV (allRes, fname)\n",
    "  Save_CSV (allDet, fname.replace('results.csv', 'detailed_results.csv'))\n",
    "  Save_CSV (allTimeDF, fname.replace('results.csv', 'Training_Time.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bmsYXuNKv_j4"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------  Comparing Against Individual Models\n",
    "\n",
    "def Compare_Models_Vs_Indiv (allRes, allRoc, allDet, fname):\n",
    "\n",
    "  if incBag:\n",
    "    Res = allRes.loc[(allRes.Type == 'Individual') | (allRes.Type == 'Bagging')]\n",
    "    Roc = allRoc.loc[(allRoc.Type == 'Individual') | (allRoc.Type == 'Bagging')]\n",
    "    Det = allDet.loc[(allDet.Type == 'Individual') | (allDet.Type == 'Bagging')]\n",
    "    Compare_Models (Res, Roc, Det, fname, 'IndxBag')\n",
    "\n",
    "  if incBos:\n",
    "    Res = allRes.loc[(allRes.Type == 'Individual') | (allRes.Type == 'Boosting')]\n",
    "    Roc = allRoc.loc[(allRoc.Type == 'Individual') | (allRoc.Type == 'Boosting')]\n",
    "    Det = allDet.loc[(allDet.Type == 'Individual') | (allDet.Type == 'Boosting')]\n",
    "    Compare_Models (Res, Roc, Det, fname, 'IndxBos')\n",
    "\n",
    "  if incTen:\n",
    "    Res = allRes.loc[(allRes.Type == 'Individual') | (allRes.Type == 'TreeBased')]\n",
    "    Roc = allRoc.loc[(allRoc.Type == 'Individual') | (allRoc.Type == 'TreeBased')]\n",
    "    Det = allDet.loc[(allDet.Type == 'Individual') | (allDet.Type == 'TreeBased')]\n",
    "    Compare_Models (Res, Roc, Det, fname, 'IndxTen')\n",
    "\n",
    "  if incSta:\n",
    "    Res = allRes.loc[(allRes.Type == 'Individual') | (allRes.Type == 'Stacking')]\n",
    "    Roc = allRoc.loc[(allRoc.Type == 'Individual') | (allRoc.Type == 'Stacking')]\n",
    "    Det = allDet.loc[(allDet.Type == 'Individual') | (allDet.Type == 'Stacking')]\n",
    "    Compare_Models (Res, Roc, Det, fname, 'IndxStack')\n",
    "\n",
    "  if incTenSta:\n",
    "    Res = allRes.loc[(allRes.Type == 'Individual') | (allRes.Type == 'TreeStacking')]\n",
    "    Roc = allRoc.loc[(allRoc.Type == 'Individual') | (allRoc.Type == 'TreeStacking')]\n",
    "    Det = allDet.loc[(allDet.Type == 'Individual') | (allDet.Type == 'TreeStacking')]\n",
    "    Compare_Models (Res, Roc, Det, fname, 'IndxTreeStack')\n",
    "\n",
    "  if incVot:\n",
    "    Res = allRes.loc[(allRes.Type == 'Individual') | (allRes.Type == 'Voting')]\n",
    "    Roc = allRoc.loc[(allRoc.Type == 'Individual') | (allRoc.Type == 'Voting')]\n",
    "    Det = allDet.loc[(allDet.Type == 'Individual') | (allDet.Type == 'Voting')]\n",
    "    Compare_Models (Res, Roc, Det, fname, 'IndxVote')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rBxolHgJ352f"
   },
   "outputs": [],
   "source": [
    "def Compare_Tune_Vs_Default (allRes, allRoc, allDet, fname):\n",
    "\n",
    "  if _CompIndTune:\n",
    "    Res = allRes.loc[(allRes.Type == 'Individual')]\n",
    "    Roc = allRoc.loc[(allRoc.Type == 'Individual')]\n",
    "    Det = allDet.loc[(allDet.Type == 'Individual')]\n",
    "    Compare_Models (Res, Roc, Det, fname, 'IndxTune')\n",
    "\n",
    "  if _CompTenTune:\n",
    "    Res = allRes.loc[(allRes.Type == 'TreeBased')]\n",
    "    Roc = allRoc.loc[(allRoc.Type == 'TreeBased')]\n",
    "    Det = allDet.loc[(allDet.Type == 'TreeBased')]\n",
    "    Compare_Models (Res, Roc, Det, fname, 'TenxTune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FELyyMZoPkya"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------  Comparing Models Across Datasets\n",
    "\n",
    "def Save_Models_All_Datasets ():\n",
    "\n",
    "  fname = getPath('Compare')\n",
    "\n",
    "  Save_Excel (allDatasetRes,  fname + getToday() + '_' + getTime() + '_AllDatasetResults.xlsx')\n",
    "  Save_Excel (allDatasetRoc,  fname + getToday() + '_' + getTime() + '_AllDatasetROC.xlsx')\n",
    "  Save_Excel (allDatasetDet,  fname + getToday() + '_' + getTime() + '_AllDatasetDet.xlsx')\n",
    "  #Save_Excel (allDatasetMat,  fname + getToday() + '_' + getTime() + '_AllDatasetMat.xlsx')\n",
    "  Save_Excel (allDatasetStat, fname + getToday() + '_' + getTime() + '_AllDatasetStatisticalAnalysis.xlsx')\n",
    "  Save_Excel (allDatasetTimeDF, fname + getToday() + '_' + getTime() + '_AllDatasetTrainingTime.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVYS27bZZ5ot"
   },
   "source": [
    "## **Statistical Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53ETl50RYF_u"
   },
   "outputs": [],
   "source": [
    "def Check_Normal (sample):\n",
    "\n",
    "  #return False\n",
    "  alpha = 0.05\n",
    "  normal = False\n",
    "\n",
    "  w , p = shapiro(sample)\n",
    "\n",
    "  if p > alpha : normal = True\n",
    "  else         : normal = False\n",
    "\n",
    "  return normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E5UyWf3vYY0F"
   },
   "outputs": [],
   "source": [
    "def Stat_Test (allDet, fname):\n",
    "\n",
    "  fname   = getPath('Compare') + fname.replace('.csv','_results.csv')\n",
    "  statRes = prepareStatDataFrame ()\n",
    "  Metric  = statMetric #'Accuracy'\n",
    "\n",
    "  statDet = pd.concat([allDet['Classifier'], allDet[Metric]], axis=1)\n",
    "  names = statDet['Classifier'].unique().tolist()\n",
    "\n",
    "  original_alpha = 0.05  # Original significance level\n",
    "  k = len(names)  # Number of models\n",
    "  m = k * (k - 1) / 2  # Number of pairwise comparisons\n",
    "\n",
    "  # Adjust the threshold p-value\n",
    "  alpha_adjusted = original_alpha / m\n",
    "    \n",
    "  for i in range(len(names)):\n",
    "\n",
    "    name = names[i]\n",
    "    nestedNames = names[i+1:]\n",
    "\n",
    "    for nested in nestedNames:\n",
    "\n",
    "      model_1 =  statDet.loc[statDet['Classifier'] == name  ][Metric]\n",
    "      model_2 =  statDet.loc[statDet['Classifier'] == nested][Metric]\n",
    "\n",
    "      m1_score = model_1.mean()\n",
    "      m2_score = model_2.mean()\n",
    "\n",
    "      win, lost = '',''\n",
    "\n",
    "      if Check_Normal (model_1) and Check_Normal (model_2):\n",
    "        test = 't-test'\n",
    "        stat, p = ttest_rel(model_1, model_2)\n",
    "      else:\n",
    "        test = 'Wilcoxon'\n",
    "        if m1_score == m2_score : stat, p = 1, 1\n",
    "        else                    : stat, p = wilcoxon (model_1, model_2)\n",
    "      if p > alpha_adjusted:\n",
    "        decision = 'Accept'\n",
    "      else:\n",
    "        decision = 'Reject'\n",
    "        if   m1_score > m2_score : win, lost = name, nested\n",
    "        else                     : win, lost = nested, name\n",
    "\n",
    "      statRes = pd.concat([statRes, pd.DataFrame([{'Classifier_1' : name,\n",
    "                                'Classifier_2' : nested,\n",
    "                                'Test'         : test,\n",
    "                                'Stat'         : stat,\n",
    "                                'p-value'      : p,\n",
    "                                'Null Hypo'    : decision,\n",
    "                                'Win'          : win,\n",
    "                                'Lost'         : lost,\n",
    "                                'Dataset'      : getDatasetName (fname)}])]\n",
    "                                ,ignore_index=True)\n",
    "\n",
    "  Save_Excel (statRes, fname.replace('results.csv', 'statistical_results.xlsx'))\n",
    "\n",
    "  return statRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stat_Test(allDet, fname, save=True):\n",
    "    fname = getPath('Compare') + fname.replace('.csv', '_results.csv')\n",
    "    statRes = prepareStatDataFrame()\n",
    "    Metric = statMetric  # 'Accuracy'\n",
    "\n",
    "    statDet = pd.concat([allDet['Classifier'], allDet[Metric]], axis=1)\n",
    "    names = statDet['Classifier'].unique().tolist()\n",
    "\n",
    "    original_alpha = 0.05  # Original significance level\n",
    "    k = len(names)  # Number of models\n",
    "\n",
    "    p_values = []\n",
    "    comparisons = []\n",
    "\n",
    "    for i, name in enumerate(names):\n",
    "        nestedNames = names[i + 1:]\n",
    "        for nested in nestedNames:\n",
    "            model_1 = statDet.loc[statDet['Classifier'] == name][Metric]\n",
    "            model_2 = statDet.loc[statDet['Classifier'] == nested][Metric]\n",
    "\n",
    "            normal_1 = Check_Normal(model_1)\n",
    "            normal_2 = Check_Normal(model_2)\n",
    "\n",
    "            if normal_1 and normal_2:  \n",
    "                test = 't-test'\n",
    "                stat, p = ttest_rel(model_1, model_2)\n",
    "            else:  \n",
    "                test = 'Wilcoxon'\n",
    "                stat, p = wilcoxon(model_1, model_2)\n",
    "\n",
    "            p_values.append(p)\n",
    "            comparisons.append((name, nested, model_1.mean(), model_2.mean()))\n",
    "\n",
    "    corrected_p_values = multipletests(p_values, alpha=original_alpha, method='bonferroni')[1]\n",
    "\n",
    "    for (name, nested, mean_1, mean_2), p in zip(comparisons, corrected_p_values):\n",
    "        decision = 'Accept' if p > original_alpha else 'Reject'\n",
    "        win, lost = ('', '') if decision == 'Accept' else ((name, nested) if mean_1 > mean_2 else (nested, name))\n",
    "        \n",
    "        new_row = {\n",
    "            'Classifier_1': name,\n",
    "            'Classifier_2': nested,\n",
    "            'Test': test,\n",
    "            'Stat': stat,\n",
    "            'p-value': p,\n",
    "            'Null Hypo': decision,\n",
    "            'Win': win,\n",
    "            'Lost': lost,\n",
    "            'Dataset': getDatasetName(fname)\n",
    "        }\n",
    "\n",
    "        statRes = pd.concat([statRes, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    if save: Save_Excel(statRes, fname.replace('results.csv', 'statistical_results.xlsx'))\n",
    "\n",
    "    return statRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVxcIh2PsaKu"
   },
   "outputs": [],
   "source": [
    "def ScottKnottESD (Det, fname):\n",
    "\n",
    "  boxplot_results = Det\n",
    "  boxplot_results = boxplot_results.rename(columns = {'Classifier': 'variable', ESDmetric: 'value'}, inplace = False)\n",
    "\n",
    "  data = sk.long2wide (boxplot_results)\n",
    "  print (data)\n",
    "\n",
    "  r_sk = sk.sk_esd(data)\n",
    "  print (r_sk[2])\n",
    "\n",
    "  r_sk[2] = fixScottNames(r_sk[2])\n",
    "\n",
    "  ranking = pd.DataFrame({'columns':r_sk[2], 'rank':list(r_sk[1])}) # long format\n",
    "\n",
    "  a_dict = dict(zip(r_sk.names, list(r_sk)))\n",
    "  data_dict = { 'nms': a_dict['nms'], 'ord': a_dict['ord']}\n",
    "  rank_val = a_dict['groups']\n",
    "\n",
    "  ord_dict = {}\n",
    "  for i in range(0,len(data_dict['nms'])):\n",
    "    ord_dict[i+1] = data_dict['nms'][i]\n",
    "\n",
    "  ord_dict2 =  {}\n",
    "  for i in range(0,len(data_dict['nms'])):\n",
    "    ord_dict2[i+1] = ord_dict[a_dict['ord'][i]]\n",
    "\n",
    "  rank_dict =  {}\n",
    "  for i in range(0,len(data_dict['nms'])):\n",
    "    rank_dict[ord_dict2[i+1]] = rank_val[i]\n",
    "\n",
    "  print (rank_dict)\n",
    "\n",
    "  rank = pd.DataFrame.from_dict(rank_dict, orient='index',  columns=['rank'])\n",
    "  rank.reset_index(inplace=True)\n",
    "  rank.rename(columns = {'index': 'variable'}, inplace = True)\n",
    "\n",
    "  boxplot_rank = pd.merge(boxplot_results, rank, on='variable')\n",
    "  boxplot_rank.rename(columns = {'variable': 'Classifier', 'value': 'AUC'}, inplace = True)\n",
    "  #print('boxplot_rank',boxplot_rank)\n",
    "\n",
    "  if _WriteFlag : Save_Excel (boxplot_rank, fname.replace('results.csv', 'ScottKnotESD_results.xlsx'))\n",
    "\n",
    "  return boxplot_rank\n",
    "\n",
    "\n",
    "def fixScottNames( rank ):\n",
    "  for i in range(len(rank)):\n",
    "    s = rank[i]\n",
    "    if 'NB.' in s:\n",
    "        s_list = list(s)\n",
    "        index = s.find('.')\n",
    "        s_list[index] = '('\n",
    "        next_dot_index = s.find('.', index + 1)\n",
    "        s_list[next_dot_index] = ')'\n",
    "        s = ''.join(s_list)\n",
    "        s = s.replace('.', '-')\n",
    "        rank[i] = s\n",
    "        continue\n",
    "    \n",
    "    if '.' in s:\n",
    "      rank[i] = s.replace('.','-')\n",
    "      continue\n",
    "            \n",
    "    if 'BS.' in s:\n",
    "      rank[i] = s.replace('BS.','BS-')\n",
    "      continue\n",
    "    if 'BG.' in s:\n",
    "      rank[i] = s.replace('BG.','BG-')\n",
    "      continue\n",
    "    if 'Stack.' in s:\n",
    "      rank[i] = s.replace('Stack.','Stack-')\n",
    "      continue\n",
    "\n",
    "\n",
    "  return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_logging(log_fname):\n",
    "    # Configure logging\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    # If you also want to log to a file, you can add a FileHandler in addition to basicConfig\n",
    "    log_filename = log_fname+'.log'\n",
    "    file_handler = logging.FileHandler(log_filename)\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "    \n",
    "    # Get the root logger and add the file handler to it\n",
    "    logging.getLogger().addHandler(file_handler)\n",
    "    \n",
    "    \n",
    "    # Example of setting a specific logger for your module\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    return logger\n",
    "\n",
    "import logging\n",
    "\n",
    "def set_logging(log_fname):\n",
    "    # Create a logger for your application\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)  # Set the log level to INFO\n",
    "    \n",
    "    # Create file handler which logs even debug messages\n",
    "    log_filename = log_fname + '.log'\n",
    "    file_handler = logging.FileHandler(log_filename)\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "    \n",
    "    # Add the handler to the logger\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    # Prevent logs from other libraries from propagating to this logger\n",
    "    logger.propagate = False\n",
    "    \n",
    "    return logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xp0O9lKLWl1l"
   },
   "source": [
    "#     **---    Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hvg47W0LL2Zg"
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------  Global Variables\n",
    "\n",
    "indClfs = prepareIndividualModels ()\n",
    "tenClfs = prepareTreeBasedEnsembles ()\n",
    "\n",
    "Res, Roc, Det, Mat, TimeDF = prepareDataFrames ()\n",
    "allRes, allRoc, allDet, allMat, allTimeDF = prepareDataFrames ()\n",
    "\n",
    "allDatasetRes, allDatasetRoc, allDatasetDet, allDatasetMat, allDatasetTimeDF = prepareDataFrames ()\n",
    "allDatasetStat = prepareStatDataFrame ()\n",
    "\n",
    "# -------------- Untuned Clfs\n",
    "unIndClfs = {}\n",
    "unTenClfs = {}\n",
    "\n",
    "# --------------- logger\n",
    "log_fname= output_dir + namestr(fnames, globals())\n",
    "logger = set_logging(log_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1Ew_JrsQhSZh",
    "outputId": "a5e9eaf0-1a15-4a02-eb4c-718b9ea4b701",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ====================     Execution Starts Here  ===================\n",
    "\n",
    "printStart()\n",
    "\n",
    "for fname in fnames:\n",
    "    \n",
    "  logger.info(f\" ====================  {fname}  ==================== \")\n",
    "\n",
    "  printString (getDatasetName(fname))\n",
    "\n",
    "  newFile = Dataset_Preprocess (fname)\n",
    "\n",
    "  ind, dep, indTun, depTun = prepareVars (newFile)\n",
    "\n",
    "  Compare_Models_Within_Dataset (ind, dep, indTun, depTun, fname)\n",
    "\n",
    "Save_Models_All_Datasets ()\n",
    "\n",
    "printDone()\n",
    "\n",
    "# ====================     Execution Ends Here  ==================="
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
